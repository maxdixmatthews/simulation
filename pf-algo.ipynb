{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5684df4e",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476dc42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/7j0s980d2w7951jcy8zc25ww08y435/T/ipykernel_52096/3242137034.py:80: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_all= pd.read_sql(sql_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   simulation_id                                               name  seed  \\\n",
      "0   2.969395e+09  nd_class=7_features=8_seed=42_samples=100000_[...  42.0   \n",
      "\n",
      "   n_classes  n_features  n_samples  \\\n",
      "0        7.0         8.0   100000.0   \n",
      "\n",
      "                                  distribution_types  \\\n",
      "0  ['Col1: normal(2.18, 2.21)', 'Col2: normal_bin...   \n",
      "\n",
      "                                        generated_nd  \\\n",
      "0  [((0, 1, 2, 3, 4, 5), (6,)), ((0, 1, 2, 3, 4),...   \n",
      "\n",
      "                                          covariates  \\\n",
      "0  [['0.3', '-0.1', '-1.4', '1', '-0.6', '0.6', '...   \n",
      "\n",
      "                                          class_freq  \\\n",
      "0  [0.16625 0.16947 0.16061 0.12791 0.11094 0.136...   \n",
      "\n",
      "                                              nd_dot  \\\n",
      "0  digraph ND {\\n  rankdir=TB;\\n  N0 [shape=ellip...   \n",
      "\n",
      "                                           nd_params        x_hash  \\\n",
      "0  {((0, 1, 2, 3, 4, 5), (6,)): ['0.3', '-0.1', '...  2.916425e+09   \n",
      "\n",
      "        y_hash  full_dataset_hash extra_condition notes  \n",
      "0  309119014.0        557143411.0                        \n",
      "{((0, 1, 2, 3, 4, 5), (6,)): ['0.3', '-0.1', '-1.4', '1', '-0.6', '0.6', '0', '0', '0.25'], ((0, 1, 2, 3, 4), (5,)): ['-2', '0', '-0.5', '1', '-0.1', '-0.2', '0', '0', '-0.2'], ((0, 1, 2, 3), (4,)): ['-1', '0', '0', '0.8', '-1', '1.1', '0', '0', '0.1'], ((0, 1, 2), (3,)): ['1.5', '0', '-0.5', '0.1', '-1', '1.5', '0', '0', '0'], ((0, 1), (2,)): ['-1', '0', '1', '0.9', '-1.2', '1', '0', '0', '0'], ((0,), (1,)): ['1', '0', '0.9', '-0.1', '-1', '1', '0', '0', '0']}\n",
      "[((0, 1, 2, 3, 4, 5), (6,)), ((0, 1, 2, 3, 4), (5,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((0,), (1,))]\n",
      "(0, 1, 2, 3, 4, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import generate_data as gd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import json \n",
    "import pandas as pd\n",
    "import generate_data as gd\n",
    "import sqlalchemy as sa\n",
    "import os\n",
    "import ast, itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "password = os.environ['ML_POSTGRESS_URL'].split(':')[2].split(\"@\")[0]\n",
    "# host = os.environ['ML_POSTGRESS_HOST']\n",
    "host = \"2m43izca46y.db.cloud.edu.au\"\n",
    "database = \"simulation\"\n",
    "engine = sa.create_engine( f\"postgresql://max:{password}@{host}\" + f\"/{database}\")\n",
    "conn = psycopg2.connect(host=host, dbname=database, user='max', password=password)\n",
    "# Import train test split \n",
    "\n",
    "# Import the train test split function from sci-kit\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn. import train_test_split\n",
    "# all_info = pd.read_clipboard()\n",
    "# all_info.to_excel(\"data/general.xlsx\", index=False)\n",
    "# all_info = pd.read_excel(\"data/general.xlsx\")\n",
    "\n",
    "sim_id = 3891194763 # 5 classes\n",
    "sim_id = 1278467595 # 4 classes\n",
    "sim_id = 2330035447 \n",
    "sim_id = 412560783\n",
    "sim_id = 2925821139\n",
    "sim_id = 623986934 # 6 classes\n",
    "sim_id = 53548897 # 4 classes\n",
    "sim_id = 3840264976 # 7 classes\n",
    "\n",
    "sim_id = 3361664654\n",
    "sim_id = 466052150\n",
    "sim_id = 2741437726\n",
    "sim_id = 2741437726\n",
    "\n",
    "sim_id = 1871628256\n",
    "\n",
    "\n",
    "sim_id = 832107684 # 12 classes\n",
    "sim_id = 2078238514 # 13 classes\n",
    "sim_id = 3074882167 # 14 classes\n",
    "sim_id = 2276442825 # 16 classes\n",
    "\n",
    "sim_id = 1801022628 # 20 classes\n",
    "sim_id = 1871628256 # 15 classes\n",
    "sim_id = 2741437726 # 11 classes\n",
    "sim_id = 832107684 # 12 classes\n",
    "\n",
    "sim_id = 4265997001 # 8 classes\n",
    "\n",
    "sim_id = 1824777786 # 19 classes\n",
    "sim_id = 726862791 # 21 classes\n",
    "sim_id = 1121062968 # 20 classes\n",
    "sim_id = 979389019 # 19 classes\n",
    "sim_id = 1871628256 # 15 classes\n",
    "sim_id = 37326262 # 10 classes\n",
    "sim_id = 774609303 # 5 classes\n",
    "sim_id = 3339144425 # 6 classes\n",
    "sim_id = 2969394773 # 7 classes\n",
    "\n",
    "run_name = \"412560783\"\n",
    "\n",
    "sql_query = f\"\"\"\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    nd_simulations_dictionary sims\n",
    "WHERE\n",
    "    simulation_id = '{sim_id}';\n",
    "\"\"\"\n",
    "df_all= pd.read_sql(sql_query, conn)\n",
    "conn.close()\n",
    "\n",
    "# single_info = all_info[all_info['simulation_id'] == sim_id]\n",
    "\n",
    "single_info = df_all[df_all['simulation_id'] == sim_id]\n",
    "print(single_info)\n",
    "for index, row in single_info.iterrows():\n",
    "    dataset_name = row['name']\n",
    "    simulation_id = row['simulation_id']\n",
    "    seed = int(float(row['seed']))\n",
    "    n_samples = int(float(row['n_samples']))\n",
    "    n_classes = int(row['n_classes'])\n",
    "    n_features = int(row['n_features'])\n",
    "    # cov_beta = json.loads(row['covariates'])\n",
    "    # intercepts = json.loads(row['intercept'])\n",
    "    x_hash = int(row['x_hash'])\n",
    "    y_hash = int(row['y_hash'])\n",
    "    full_dataset_hash = int(row['full_dataset_hash'])\n",
    "    extra_conditions = str(row['extra_condition'])\n",
    "    # given_best_tree = row['selected_tree']\n",
    "    cov = ast.literal_eval(row['covariates'])\n",
    "    nd_structure = ast.literal_eval(row['generated_nd'])\n",
    "    nd_params = ast.literal_eval(row['nd_params'])\n",
    "    print(nd_params)\n",
    "\n",
    "    best_prev_tree = tuple(gd.bfs_splits(tuple(nd_structure)))\n",
    "\n",
    "    # X, y, dist_types, proba, freq = gd.generate_mlr_data(seed, n_features, n_samples, n_classes, cov_beta, intercepts, extra_conditions=extra_conditions)\n",
    "    X, y, dist_types, freq = gd.generate_nd_data(seed, n_features, n_samples, n_classes, cov, nd_structure, nd_params=nd_params, extra_conditions=extra_conditions)\n",
    "    # gd.check_hashes(X, y, x_hash, y_hash, full_dataset_hash)\n",
    "    df = pd.DataFrame(X, columns=[f\"p{i+1}\" for i in range(n_features)])\n",
    "    # df['Y'] = y\n",
    "    X, X_test, y, y_test = train_test_split(df, y, test_size=0.2, random_state=seed)\n",
    "    categories = tuple(np.unique(y))\n",
    "    print(categories)\n",
    "\n",
    "def v2_defined_all_trees(n: int):\n",
    "    \"\"\"Return a list of all ND trees over labels 0..n-1 (each tree is a tuple of splits).\"\"\"\n",
    "    return list(_gen(tuple(range(n))))\n",
    "\n",
    "def _gen(labels):\n",
    "    labels = tuple(sorted(labels))\n",
    "    if len(labels) <= 1:\n",
    "        yield ()\n",
    "        return\n",
    "    s = labels[0]  # canonical rule: the smallest label stays on the 'left' side\n",
    "    for r in range(1, len(labels)):\n",
    "        for rest in combinations(labels[1:], r - 1):\n",
    "            left  = tuple(sorted((s,) + rest))\n",
    "            right = tuple(x for x in labels if x not in left)\n",
    "            # normalize pair so children are unordered: larger side first, then lexicographic\n",
    "            pair  = tuple(sorted((left, right), key=lambda t: (-len(t), t)))\n",
    "\n",
    "            L = ((),) if len(left)  == 1 else tuple(_gen(left))\n",
    "            R = ((),) if len(right) == 1 else tuple(_gen(right))\n",
    "\n",
    "            for lt in L:\n",
    "                for rt in R:\n",
    "                    # pre-order DFS (shortest code). Swap to (lt+rt+(pair,)) if you want post-order.\n",
    "                    yield (pair,) + lt + rt\n",
    "\n",
    "# trees_defined = v2_defined_all_trees(len(categories))\n",
    "# trees_defined\n",
    "# len(set(trees_defined))\n",
    "# tree_dict = {\"tree\": trees_defined}\n",
    "# df_trees = pd.DataFrame(tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425c7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((1, 2), (0,)), ((2,), (1,))), 0.0)\n",
      "((((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 2), (1,)), ((2,), (0,))), 0.027777777777777776, 0.027777777777777776)\n"
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "import math\n",
    "from scipy.special import factorial2\n",
    "import random\n",
    "from numpy.random import Generator, PCG64\n",
    "# General imports\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import Generator, PCG64\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
    "import warnings\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import Category10\n",
    "\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, save, output_file\n",
    "from bokeh.layouts import column as bcol, row as brow   # <-- alias to avoid shadowing\n",
    "from bokeh.models import ColumnDataSource, Div\n",
    "from bokeh.palettes import Category10\n",
    "from graphviz import Source\n",
    "\n",
    "def viz_mh_trace(traces, burn=0, top_k=20, html_path=None, best_known=None, loglik_fn=None, notes=\"\"):\n",
    "    try: L = min(len(t) for t in traces); arr = np.stack([np.asarray(t[:L], object) for t in traces], 0)\n",
    "    except Exception: arr = np.asarray(traces, object)[None, :]\n",
    "\n",
    "    flat = arr.ravel()\n",
    "    uniq_trees, inv = np.unique(flat, return_inverse=True)\n",
    "    arr_id = inv.reshape(arr.shape)\n",
    "\n",
    "    accs = [(tr[1:] != tr[:-1]).mean() if tr.size > 1 else np.nan for tr in arr_id]\n",
    "    overall = float(np.nanmean(accs)) if len(accs) else np.nan\n",
    "\n",
    "    post = arr_id[:, burn:].ravel()\n",
    "    uniq, counts = np.unique(post, return_counts=True)\n",
    "    order = counts.argsort()[::-1][:top_k]\n",
    "    top_ids, probs = uniq[order], counts[order] / counts.sum()\n",
    "    labels = [str(int(i)) for i in top_ids]\n",
    "    top_trees = [uniq_trees[int(i)] for i in top_ids]\n",
    "\n",
    "    _ll = (lambda T: float(loglik_fn(T))) if loglik_fn else (lambda T: None)\n",
    "    bk_ll = _ll(best_known) if (best_known is not None) else None\n",
    "    top_lls = [_ll(T) for T in top_trees]\n",
    "    if loglik_fn and any(ll is not None for ll in top_lls):\n",
    "        j = int(np.nanargmax(top_lls)); best_found_id, best_found_ll, best_found_tree = int(top_ids[j]), top_lls[j], top_trees[j]\n",
    "    else:\n",
    "        best_found_id, best_found_ll, best_found_tree = int(top_ids[0]), None, top_trees[0]\n",
    "\n",
    "    pal = Category10[10]\n",
    "\n",
    "    p1 = figure(title=\"Trace (tree id)\", width=720, height=240,\n",
    "                tools=\"xpan,xwheel_zoom,reset,hover\",\n",
    "                tooltips=[(\"iter\",\"@it\"),(\"id\",\"@idx\"),(\"chain\",\"@ch\")])\n",
    "    for c, tr in enumerate(arr_id):\n",
    "        it = np.arange(tr.size); src = ColumnDataSource(dict(it=it, idx=tr, ch=[str(c)]*tr.size))\n",
    "        p1.line('it','idx', source=src, alpha=0.85, line_width=2, color=pal[c%10])\n",
    "        p1.circle('it','idx', source=src, size=3, alpha=0.6, color=pal[c%10])\n",
    "\n",
    "    def _acf(x, L=50):\n",
    "        x = np.asarray(x, float); x -= x.mean(); den = (x*x).sum() or 1.0\n",
    "        K = min(L, max(1, x.size-1)); return np.array([1.0] + [float(np.dot(x[:-k], x[k:]) / den) for k in range(1, K+1)])\n",
    "    p2 = figure(title=\"Autocorrelation\", width=720, height=220, x_axis_label=\"lag\",\n",
    "                tools=\"xpan,xwheel_zoom,reset,hover\", tooltips=[(\"lag\",\"@lag\"),(\"acf\",\"@acf{0.000}\")])\n",
    "    for c, tr in enumerate(arr_id):\n",
    "        ac = _acf(tr); src = ColumnDataSource(dict(lag=np.arange(ac.size), acf=ac))\n",
    "        p2.line('lag','acf', source=src, line_width=2, alpha=0.9, color=pal[c%10])\n",
    "\n",
    "    src2 = ColumnDataSource(dict(idx=labels, p=probs))\n",
    "    p3 = figure(x_range=labels, title=f\"Posterior mass (top {top_k})\", width=720, height=260,\n",
    "                tools=\"hover\", tooltips=[(\"id\",\"@idx\"),(\"mass\",\"@p{0.000}\")])\n",
    "    p3.vbar(x='idx', top='p', source=src2, width=0.9)\n",
    "\n",
    "    ac_labels = [f\"ch{c}\" for c in range(arr_id.shape[0])]\n",
    "    src3 = ColumnDataSource(dict(ch=ac_labels, a=[float(a) for a in accs]))\n",
    "    p4 = figure(x_range=ac_labels, title=f\"Acceptance (overall {overall:.3f})\", width=720, height=200,\n",
    "                tools=\"hover\", tooltips=[(\"chain\",\"@ch\"),(\"acc\",\"@a{0.000}\")])\n",
    "    p4.vbar(x='ch', top='a', source=src3, width=0.7)\n",
    "\n",
    "    def _fmt_tree(t): s = str(t); return s if len(s) <= 800 else s[:797] + \"…\"\n",
    "    def _fmt_tree_lines(t):\n",
    "        try: return \"\\n\".join(str((tuple(L), tuple(R))) for (L,R) in t)\n",
    "        except Exception: return _fmt_tree(t)\n",
    "\n",
    "    def _dot_svg(T):\n",
    "        try:\n",
    "            by = {frozenset((*L,*R)):(L,R) for (L,R) in T}\n",
    "            root = set().union(*[set(L)|set(R) for (L,R) in T])\n",
    "            dot = [\"digraph ND{rankdir=TB;node[shape=box,fontsize=9];\"]; seen=set()\n",
    "            def rec(U):\n",
    "                Ufs=frozenset(U)\n",
    "                if Ufs in seen: return\n",
    "                seen.add(Ufs)\n",
    "                name=\"n_\"+\"_\".join(map(str,sorted(U))); lab=\"{\"+\",\".join(map(str,sorted(U)))+\"}\"\n",
    "                dot.append(f'{name}[label=\"{lab}\"];')\n",
    "                if len(U)<=1: return\n",
    "                L,R=by[frozenset(U)]\n",
    "                for V in (set(L),set(R)):\n",
    "                    v=\"n_\"+\"_\".join(map(str,sorted(V))); dot.append(f\"{name}->{v};\"); rec(V)\n",
    "            rec(root); dot.append(\"}\")\n",
    "            return Source(\"\\n\".join(dot)).pipe(format=\"svg\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            return \"<!-- graphviz failed -->\"\n",
    "\n",
    "    bk_svg = _dot_svg(best_known) if best_known is not None else \"\"\n",
    "    bf_svg = _dot_svg(best_found_tree) if best_found_tree is not None else \"\"\n",
    "\n",
    "    bk_txt = \"\" if best_known is None else f\"<h3>Best-known</h3><pre>{_fmt_tree_lines(best_known)}</pre><br>\"\n",
    "    bf_txt = f\"<h3>Best-found (id {best_found_id}{'' if best_found_ll is None else f', ll {best_found_ll:.3f}'})</h3><pre>{_fmt_tree_lines(best_found_tree)}</pre>\"\n",
    "\n",
    "    tops = \"\".join(f\"<li><b>id {int(i)}</b> (p={p:.3f}{'' if (top_lls[k] is None) else f', ll {top_lls[k]:.3f}'})<br><code>{_fmt_tree(top_trees[k])}</code></li>\"\n",
    "                   for k,(i,p) in enumerate(zip(top_ids, probs)))\n",
    "\n",
    "    div_summary = Div(text=f\"<h2>MH Summary</h2>{'' if bk_ll is None else f'<p><b>Best-known ll:</b> {bk_ll:.3f}</p>'}{notes}\")\n",
    "    div_best = Div(text=f\"<table><tr>\"\n",
    "                        f\"<td valign='top'>{bk_txt}{bk_svg}</td>\"\n",
    "                        f\"<td style='width:20px'></td>\"\n",
    "                        f\"<td valign='top'>{bf_txt}{bf_svg}</td>\"\n",
    "                        f\"</tr></table>\")\n",
    "    div_list = Div(text=f\"<h3>Top {top_k} trees</h3><ol>{tops}</ol>\")\n",
    "\n",
    "    layout = brow(bcol(p1, p2, p3, p4), bcol(div_summary, div_best, div_list))  # <-- use aliases here\n",
    "    if html_path: output_file(html_path, title=\"MH Diagnostics\"); save(layout)\n",
    "    else: show(layout)\n",
    "\n",
    "    return {\n",
    "        \"ids_map\": {int(i): uniq_trees[int(i)] for i in top_ids},\n",
    "        \"top_ids\": top_ids, \"top_probs\": probs, \"top_trees\": top_trees,\n",
    "        \"acc\": accs, \"overall\": overall,\n",
    "        \"best_found_id\": best_found_id, \"best_found_ll\": best_found_ll, \"best_found_tree\": best_found_tree,\n",
    "        \"best_known_ll\": bk_ll\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _canon_split(left_set, right_set):\n",
    "    \"\"\"Canonicalize a split (A,B) so (A,B)==(B,A) for hashing/caching.\"\"\"\n",
    "    A = tuple(sorted(left_set))\n",
    "    B = tuple(sorted(right_set))\n",
    "    return (A, B) if A <= B else (B, A)\n",
    "\n",
    "def tree_signatures(tree):\n",
    "    \"\"\"\n",
    "    Tree is a tuple/list of splits from defined_all_trees:\n",
    "      ((A1,B1), (A2,B2), ..., (A_{n-1},B_{n-1}))\n",
    "    Return a frozenset of canonicalized splits.\n",
    "    \"\"\"\n",
    "    return frozenset(_canon_split(a, b) for (a, b) in tree)\n",
    "\n",
    "T = (((0, 1), (2, 3)), ((0,), (1,)), ((2,), (3,)))\n",
    "T = (((0, 1, 2), (3,)), ((0, 1), (2,)), ((0,), (1,)))\n",
    "T = (((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((0,), (1,)))\n",
    "\n",
    "rng = Generator(PCG64(42))\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_splits(tree):\n",
    "    # normalise each split: sort labels; bigger side left (tie → lexicographic)\n",
    "    S = [ (tuple(sorted(L)), tuple(sorted(R))) for (L,R) in tree ]\n",
    "    S = [ (max(a,b, key=lambda t:(len(t), t)), min(a,b, key=lambda t:(len(t), t))) for a,b in S ]\n",
    "\n",
    "    # map subset → split; find root label set\n",
    "    by = { frozenset((*L, *R)): (L, R) for (L, R) in S }\n",
    "    root = set().union(*[ set(L)|set(R) for (L,R) in S ])\n",
    "\n",
    "    # BFS order\n",
    "    out, q = [], deque([root])\n",
    "    while q:\n",
    "        U = q.popleft()\n",
    "        if len(U) <= 1: \n",
    "            continue\n",
    "        L, R = by[frozenset(U)]\n",
    "        out.append((L, R))\n",
    "        q.append(set(L)); q.append(set(R))\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "def local_swap(T, rg):\n",
    "    all_splits = list(T)\n",
    "    eligible_splits = [x for x in all_splits if len(x[0]) > 1 or len(x[1]) > 1]\n",
    "    # print(eligible_splits)\n",
    "    node_to_perform_on = random.sample(eligible_splits, 1)[0]\n",
    "    # local_item_swap\n",
    "    L = list(node_to_perform_on[0])\n",
    "    R = list(node_to_perform_on[1])\n",
    "    left_swap = random.sample(L, 1)[0]\n",
    "    right_swap = random.sample(R, 1)[0]\n",
    "    # print(f\"Swapping {left_swap} with {right_swap}\")\n",
    "    # print(f\"Original node: {node_to_perform_on}\")\n",
    "    L.remove(left_swap)\n",
    "    R.remove(right_swap)\n",
    "    new_L = L + [right_swap]\n",
    "    new_R = R + [left_swap]\n",
    "    new_node = (tuple(new_L), tuple(new_R))\n",
    "\n",
    "    # Now cascade the changes down the tree\n",
    "    T = list(T)\n",
    "    S   = set(new_node[0]) | set(new_node[1])\n",
    "    U = [set(L)|set(R) for (L,R) in T]\n",
    "    desc_idx = [k for k in range(0, len(T)) if U[k].issubset(S)]\n",
    "\n",
    "    for k in desc_idx:\n",
    "        L, R = T[k]\n",
    "        A = np.concatenate([np.asarray(L), np.asarray(R)])  \n",
    "        ma, mb = (A == left_swap), (A == right_swap)\n",
    "        if ma.any() or mb.any():\n",
    "            A[ma], A[mb] = right_swap, left_swap\n",
    "            nL = len(L)\n",
    "            c = [sorted(A[:nL]), sorted(A[nL:])]\n",
    "            c.sort(key=len, reverse=True)\n",
    "            l_sub = c[0]\n",
    "            r_sub = c[1]\n",
    "            T[k] = (tuple(l_sub), tuple(r_sub))\n",
    "\n",
    "    # T = gd.breadth_first_splits(tuple(T))\n",
    "\n",
    "    return bfs_splits(T), 0.0\n",
    "\n",
    "def entire_node_swap(T, rg):\n",
    "    total_possible = 0\n",
    "    # all_splits = gd.breadth_first_splits(T)\n",
    "    all_splits = list(T)\n",
    "    eligible_splits = [x for x in all_splits if len(x[0]) > 1 or len(x[1]) > 1]\n",
    "    node_to_perform_on = random.sample(eligible_splits, 1)[0]\n",
    "    total_possible += math.comb(len(eligible_splits), 1)\n",
    "    node_choices = len(eligible_splits)\n",
    "    S = set(node_to_perform_on[0]) | set(node_to_perform_on[1])\n",
    "    original_nodes = [ (L,R) for (L,R) in all_splits if (set(L)|set(R)).issubset(S) and (len(L)+len(R) >= 2) ]\n",
    "\n",
    "    # entire node swap\n",
    "    L = list(node_to_perform_on[0])\n",
    "    R = list(node_to_perform_on[1])\n",
    "    all_classes = set(L + R)\n",
    "    number_of_classes = len(all_classes)\n",
    "    left_split_size = random.choice(range(1,number_of_classes))\n",
    "    # total_possible += factorial2(2*number_of_classes - 3)\n",
    "    new_L = sorted(random.sample(list(all_classes), left_split_size))\n",
    "    new_R = sorted(list(set(all_classes) - set(new_L)))\n",
    "    new_node = (tuple(new_L), tuple(new_R))\n",
    "    T = list(T)\n",
    "    T.remove(node_to_perform_on)\n",
    "    T.append(new_node)\n",
    "    g_top = 1.0 / ((number_of_classes - 1) * math.comb(number_of_classes, left_split_size))\n",
    "\n",
    "    # Now cascade the changes down the tree\n",
    "    T = list(T)\n",
    "    S   = set(new_node[0]) | set(new_node[1])\n",
    "\n",
    "    rm = {i for i,(L,R) in enumerate(T) if (set(L)|set(R)).issubset(S)}\n",
    "    T  = [x for i,x in enumerate(T) if i not in rm]\n",
    "\n",
    "    def grow(classes):\n",
    "        classes = list(classes)\n",
    "        n = len(classes)\n",
    "        if n <= 1:\n",
    "            return [], 1.0\n",
    "        l_split_size = random.choice(range(1,n))                  \n",
    "        A = set(random.sample(classes, l_split_size))\n",
    "        B = set(classes) - A\n",
    "        c = [A, B]\n",
    "        c.sort(key=len, reverse=True)\n",
    "        A = c[0]\n",
    "        B = c[1]\n",
    "        left_splits,  pL = grow(A)\n",
    "        right_splits, pR = grow(B)\n",
    "        node = (tuple(sorted(A)), tuple(sorted(B)))\n",
    "        return [node] + left_splits + right_splits, (1.0 / ((n - 1) * math.comb(n, l_split_size))) * pL * pR\n",
    "    \n",
    "    left_desc,  p_left  = grow(new_node[0])\n",
    "    right_desc, p_right = grow(new_node[1])\n",
    "    T += [new_node] + left_desc + right_desc\n",
    "    # T = gd.breadth_first_splits(tuple(T))\n",
    "\n",
    "    # print(f\"Swapping entire node {node_to_perform_on} with {new_node}\")\n",
    "    # 5) full probability g(T*|T)\n",
    "    E_star = sum(1 for L,R in T if len(L)>1 or len(R)>1)\n",
    "    g_rev = 1.0 / E_star\n",
    "    for (L,R) in original_nodes:              \n",
    "        mm, kk = len(L)+len(R), len(L)       \n",
    "        g_rev *= 1.0 / ((mm - 1) * math.comb(mm, kk))\n",
    "\n",
    "    g_fwd = (1.0 / node_choices) * g_top * p_left * p_right\n",
    "    return bfs_splits(T), g_fwd, g_rev\n",
    "\n",
    "print(local_swap(T, rng))\n",
    "print(entire_node_swap(T, rng))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8896b5",
   "metadata": {},
   "source": [
    "# 1 - Particle Filtering Basic\n",
    "\n",
    "Just a first try at particle filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915364ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bfs_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m fk \u001b[38;5;241m=\u001b[39m NDFK(X, y, batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     96\u001b[0m pf \u001b[38;5;241m=\u001b[39m particles\u001b[38;5;241m.\u001b[39mSMC(fk\u001b[38;5;241m=\u001b[39mfk, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, resampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystematic\u001b[39m\u001b[38;5;124m'\u001b[39m, ESSrmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, store_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mpf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mpf\u001b[38;5;241m.\u001b[39mW)\n\u001b[1;32m     99\u001b[0m top_trees \u001b[38;5;241m=\u001b[39m [_normT(pf\u001b[38;5;241m.\u001b[39mX[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m order[:\u001b[38;5;241m20\u001b[39m]]\n",
      "File \u001b[0;32m~/Documents/code/simulation/venv/lib/python3.10/site-packages/particles/utils.py:85\u001b[0m, in \u001b[0;36mtimer.<locals>.timed_method\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtimed_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     84\u001b[0m     starting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 85\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m starting_time\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/code/simulation/venv/lib/python3.10/site-packages/particles/core.py:408\u001b[0m, in \u001b[0;36mSMC.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs particle filter until completion.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    Note: this class implements the iterator protocol. This makes it\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    command only.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/simulation/venv/lib/python3.10/site-packages/particles/core.py:374\u001b[0m, in \u001b[0;36mSMC.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_particles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_auxiliary_weights()  \u001b[38;5;66;03m# APF\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/simulation/venv/lib/python3.10/site-packages/particles/core.py:321\u001b[0m, in \u001b[0;36mSMC.generate_particles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfk\u001b[38;5;241m.\u001b[39mGamma0(u)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM0\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m, in \u001b[0;36mNDFK.M0\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mM0\u001b[39m(\u001b[38;5;28mself\u001b[39m, N):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([_normT(bfs_splits(init_tree_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng))) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N)], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mM0\u001b[39m(\u001b[38;5;28mself\u001b[39m, N):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([_normT(\u001b[43mbfs_splits\u001b[49m(init_tree_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng))) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N)], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bfs_splits' is not defined"
     ]
    }
   ],
   "source": [
    "# pip install particles\n",
    "import numpy as np, random, math, particles\n",
    "from numpy.random import Generator, PCG64\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def init_tree_n(n_classes, rng):\n",
    "\n",
    "    def grow(classes):\n",
    "        classes = list(classes)\n",
    "        n = len(classes)\n",
    "        if n <= 1:\n",
    "            return []\n",
    "        l_split_size = random.choice(range(1,n))                  \n",
    "        A = set(random.sample(classes, l_split_size))\n",
    "        B = set(classes) - A\n",
    "        left_splits = grow(A)\n",
    "        right_splits= grow(B)\n",
    "        node = (tuple(sorted(A)), tuple(sorted(B)))\n",
    "        return [node] + left_splits + right_splits\n",
    "\n",
    "    return grow(range(0, n_classes))\n",
    "\n",
    "def split_loglik_star_sm(X, y, split, maxiter=200, eps=1e-12):\n",
    "    \"\"\"\n",
    "    ℓ*(split) using statsmodels Logit (unpenalized MLE).\n",
    "    Returns the node log-likelihood at the MLE (float).\n",
    "    Degenerate targets (all 0/1) → return 0.0 (supremum in the limit).\n",
    "    \"\"\"\n",
    "    A, B = split\n",
    "    mask = np.isin(y, A + B)\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    s = np.isin(y[mask], B).astype(int)\n",
    "\n",
    "    if s.min() == s.max():\n",
    "        return 0.0\n",
    "    Xsub = sm.add_constant(X[mask], has_constant=\"add\")\n",
    "    try: \n",
    "        # res = sm.Logit(s, Xsub).fit(disp=0, maxiter=maxiter, method=\"newton\", tol=1e-6)\n",
    "        # return float(res.llf)\n",
    "        # res = sm.Logit(s, Xsub).fit(disp=0, maxiter=maxiter, method=\"lbfgs\", tol=1e-6)\n",
    "        # Direct log-likelihood at the MLE:\n",
    "        lr = LogisticRegression(penalty=None, solver=\"newton-cholesky\", max_iter=maxiter)\n",
    "        # lr = LogisticRegression(penalty=None, solver=\"newton-cholesky\", max_iter=maxiter)\n",
    "        lr.fit(X[mask], s)\n",
    "        p = lr.predict_proba(X[mask])[:, 1]\n",
    "        return float(-log_loss(s, p, normalize=False))\n",
    "    except PerfectSeparationError:\n",
    "        print(\"Perfect Separation Error.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        # Any numerical prob: treat as saturated/flat\n",
    "        print(\"Failed to do normal so we are fitting regularized.\")\n",
    "        p = np.clip(sm.Logit(s, Xsub).fit_regularized(alpha=1e-6, L1_wt=0.0, maxiter=maxiter).predict(), eps, 1-eps)\n",
    "        return float((s*np.log(p) + (1-s)*np.log(1-p)).sum())\n",
    "    \n",
    "import numpy as np, random, particles\n",
    "from numpy.random import Generator, PCG64\n",
    "def _tree_ll_batch(Xb, yb, T):\n",
    "    return sum(split_loglik_star_sm(Xb, yb, s) for s in tree_signatures(T))\n",
    "\n",
    "_normT = lambda T: tuple((tuple(L), tuple(R)) for (L, R) in T)  # <-- ensure pure tuples\n",
    "\n",
    "class NDFK(particles.FeynmanKac):\n",
    "    def __init__(self, X, y, batches=10, lam=0.6, n_classes=None, seed=42):\n",
    "        self.X = np.asarray(getattr(X,'values',X)); self.y = np.asarray(getattr(y,'values',y))\n",
    "        self.lam = lam; self.rng = Generator(PCG64(seed)); random.seed(seed)\n",
    "        idx = np.arange(len(self.y)); self.rng.shuffle(idx)\n",
    "        sp = np.array_split(idx, batches) if isinstance(batches,int) else [np.asarray(b,int) for b in batches]\n",
    "        self.splits = [np.asarray(b, int).ravel() for b in sp]\n",
    "        self.T = len(self.splits); self.K = n_classes or int(np.unique(self.y).size)\n",
    "\n",
    "    def M0(self, N):\n",
    "        return np.array([_normT(bfs_splits(init_tree_n(self.K, self.rng))) for _ in range(N)], dtype=object)\n",
    "\n",
    "    def M(self, t, xp):\n",
    "        out = []\n",
    "        for T in xp:\n",
    "            T = _normT(T)  # <-- make tuples\n",
    "            if self.rng.uniform() < self.lam:\n",
    "                T, _   = local_swap(T, self.rng)\n",
    "            else:\n",
    "                T, *_  = entire_node_swap(T, self.rng)\n",
    "            out.append(_normT(T))\n",
    "        return np.asarray(out, dtype=object)\n",
    "\n",
    "    def logG(self, t, xp, x):\n",
    "        I = self.splits[t] \n",
    "        Xb, yb = self.X[I], self.y[I]\n",
    "        return np.array([_tree_ll_batch(Xb, yb, T) for T in x], float)\n",
    "\n",
    "# ---- quick run ----\n",
    "fk = NDFK(X, y, batches=10, lam=0.6, seed=42)\n",
    "pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.05, store_history=False)\n",
    "pf.run()\n",
    "order = np.argsort(-pf.W)\n",
    "top_trees = [_normT(pf.X[i]) for i in order[:20]]\n",
    "print(top_trees)\n",
    "\n",
    "# ---- step-by-step ----\n",
    "# fk = NDFK(X, y, batches=10, lam=0.6, seed=42)\n",
    "# pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.5, store_history=False)\n",
    "# pf.initialise()\n",
    "# for _ in range(fk.T): pf.step()\n",
    "# order = np.argsort(-pf.W); top_trees = [pf.X[i] for i in order[:20]]\n",
    "\n",
    "print(\"Top pf tree is:\", _normT(pf.X[order[0]]))\n",
    "if bfs_splits(best_prev_tree) in top_trees:\n",
    "    print(\"The best tree is found inside top trees!\")\n",
    "else:\n",
    "    print(\"The best tree is NOT found by particle filtering.\")\n",
    "    print(\"Best tree is:\", best_prev_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install particles\n",
    "import numpy as np, random, math, particles\n",
    "from numpy.random import Generator, PCG64\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def init_tree_n(n_classes, rng):\n",
    "\n",
    "    def grow(classes):\n",
    "        classes = list(classes)\n",
    "        n = len(classes)\n",
    "        if n <= 1:\n",
    "            return []\n",
    "        l_split_size = random.choice(range(1,n))                  \n",
    "        A = set(random.sample(classes, l_split_size))\n",
    "        B = set(classes) - A\n",
    "        left_splits = grow(A)\n",
    "        right_splits= grow(B)\n",
    "        node = (tuple(sorted(A)), tuple(sorted(B)))\n",
    "        return [node] + left_splits + right_splits\n",
    "\n",
    "    return grow(range(0, n_classes))\n",
    "\n",
    "def split_loglik_star_sm(X, y, split, maxiter=200, eps=1e-12):\n",
    "    \"\"\"\n",
    "    ℓ*(split) using statsmodels Logit (unpenalized MLE).\n",
    "    Returns the node log-likelihood at the MLE (float).\n",
    "    Degenerate targets (all 0/1) → return 0.0 (supremum in the limit).\n",
    "    \"\"\"\n",
    "    A, B = split\n",
    "    mask = np.isin(y, A + B)\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    s = np.isin(y[mask], B).astype(int)\n",
    "\n",
    "    if s.min() == s.max():\n",
    "        return 0.0\n",
    "    Xsub = sm.add_constant(X[mask], has_constant=\"add\")\n",
    "    try: \n",
    "        # res = sm.Logit(s, Xsub).fit(disp=0, maxiter=maxiter, method=\"newton\", tol=1e-6)\n",
    "        # return float(res.llf)\n",
    "        # res = sm.Logit(s, Xsub).fit(disp=0, maxiter=maxiter, method=\"lbfgs\", tol=1e-6)\n",
    "        # Direct log-likelihood at the MLE:\n",
    "        lr = LogisticRegression(penalty=None, solver=\"newton-cholesky\", max_iter=maxiter)\n",
    "        # lr = LogisticRegression(penalty=None, solver=\"newton-cholesky\", max_iter=maxiter)\n",
    "        lr.fit(X[mask], s)\n",
    "        p = lr.predict_proba(X[mask])[:, 1]\n",
    "        return float(-log_loss(s, p, normalize=False))\n",
    "    except PerfectSeparationError:\n",
    "        print(\"Perfect Separation Error.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        # Any numerical prob: treat as saturated/flat\n",
    "        print(\"Failed to do normal so we are fitting regularized.\")\n",
    "        p = np.clip(sm.Logit(s, Xsub).fit_regularized(alpha=1e-6, L1_wt=0.0, maxiter=maxiter).predict(), eps, 1-eps)\n",
    "        return float((s*np.log(p) + (1-s)*np.log(1-p)).sum())\n",
    "    \n",
    "import numpy as np, random, particles\n",
    "from numpy.random import Generator, PCG64\n",
    "def _tree_ll_batch(Xb, yb, T):\n",
    "    return sum(split_loglik_star_sm(Xb, yb, s) for s in tree_signatures(T))\n",
    "\n",
    "_normT = lambda T: tuple((tuple(L), tuple(R)) for (L, R) in T)  # <-- ensure pure tuples\n",
    "\n",
    "class NDFK(particles.FeynmanKac):\n",
    "    def __init__(self, X, y, batches=10, lam=0.6, n_classes=None, seed=42):\n",
    "        self.X = np.asarray(getattr(X,'values',X)); self.y = np.asarray(getattr(y,'values',y))\n",
    "        self.lam = lam; self.rng = Generator(PCG64(seed)); random.seed(seed)\n",
    "        idx = np.arange(len(self.y)); self.rng.shuffle(idx)\n",
    "        sp = np.array_split(idx, batches) if isinstance(batches,int) else [np.asarray(b,int) for b in batches]\n",
    "        self.splits = [np.asarray(b, int).ravel() for b in sp]\n",
    "        self.T = len(self.splits); self.K = n_classes or int(np.unique(self.y).size)\n",
    "\n",
    "    def M0(self, N):\n",
    "        return np.array([_normT(bfs_splits(init_tree_n(self.K, self.rng))) for _ in range(N)], dtype=object)\n",
    "\n",
    "    def M(self, t, xp):\n",
    "        out = []\n",
    "        for T in xp:\n",
    "            T = _normT(T)  # <-- make tuples\n",
    "            if self.rng.uniform() < self.lam:\n",
    "                T, _   = local_swap(T, self.rng)\n",
    "            else:\n",
    "                T, *_  = entire_node_swap(T, self.rng)\n",
    "            out.append(_normT(T))\n",
    "        return np.asarray(out, dtype=object)\n",
    "\n",
    "    def logG(self, t, xp, x):\n",
    "        I = self.splits[t] \n",
    "        Xb, yb = self.X[I], self.y[I]\n",
    "        return np.array([_tree_ll_batch(Xb, yb, T) for T in x], float)\n",
    "\n",
    "# ---- quick run ----\n",
    "fk = NDFK(X, y, batches=10, lam=0.6, seed=42)\n",
    "pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.5, store_history=False)\n",
    "pf.run()\n",
    "order = np.argsort(-pf.W)\n",
    "top_trees = [_normT(pf.X[i]) for i in order[:20]]\n",
    "print(top_trees)\n",
    "\n",
    "# ---- step-by-step ----\n",
    "# fk = NDFK(X, y, batches=10, lam=0.6, seed=42)\n",
    "# pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.5, store_history=False)\n",
    "# pf.initialise()\n",
    "# for _ in range(fk.T): pf.step()\n",
    "# order = np.argsort(-pf.W); top_trees = [pf.X[i] for i in order[:20]]\n",
    "\n",
    "print(\"Top pf tree is:\", _normT(pf.X[order[0]]))\n",
    "if bfs_splits(best_prev_tree) in top_trees:\n",
    "    print(\"The best tree is found inside top trees!\")\n",
    "else:\n",
    "    print(\"The best tree is NOT found by particle filtering.\")\n",
    "    print(\"Best tree is:\", best_prev_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586e8ac",
   "metadata": {},
   "source": [
    "# 2 - Particle Filtering Build Up \n",
    "\n",
    "# Layer-by-Layer SMC for Nested Dichotomies (uniform proposal)\n",
    "\n",
    "**Classes.** Let $\\mathcal{C}=\\{0,1,\\ldots,K-1\\}$.\n",
    "\n",
    "**State at time $t$.** $x_t=(S_t,O_t)$ where  \n",
    "- $S_t$ is a tuple of internal splits so far; each split is an unordered pair $(A,B)$ with $A\\cap B=\\varnothing$, $A\\cup B=L$ for some leaf $L$.  \n",
    "- $O_t$ is a tuple of open (unsplit) leaves; each leaf is a subset of $\\mathcal{C}$.\n",
    "\n",
    "**Horizon.** $T=K-1$ (add one split per step until all leaves are singletons).\n",
    "\n",
    "**Initial state.** $S_0=\\varnothing,\\quad O_0=\\{\\mathcal{C}\\}$.\n",
    "\n",
    "**Proposal $M_t$ (structure only).**\n",
    "1. Pick a leaf $L\\in O_{t-1}$ uniformly.  \n",
    "2. Sample a uniform **unordered**, non-trivial binary partition $(A,B)$ of $L$.  \n",
    "3. Update\n",
    "\n",
    "$S_t=S_{t-1}\\cup\\{(A,B)\\},\\qquad\n",
    "O_t=\\big(O_{t-1}\\setminus\\{L\\}\\big)\\cup\\{A:\\lvert A\\rvert>1\\}\\cup\\{B:\\lvert B\\rvert>1\\}.\n",
    "$\n",
    "\n",
    "**Weights (potential) $G_t$ with batch $I_t$.**  \n",
    "Define\n",
    "\n",
    "$\\ell^*=\\sum_{(A,B)\\in S}\\ell(A,B;X_I,y_I)$,\n",
    "\n",
    "where $\\ell^*$ is the node log-likelihood at the MLE (degenerate $0/1$ target $\\Rightarrow 0$).  \n",
    "Then\n",
    "$\\log G_t=\\ell(S_t;I_t)-\\ell(S_{t-1};I_t).$\n",
    "\n",
    "\n",
    "**Resampling.** Standard SMC (e.g., systematic) with an ESS threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce6e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, save, output_file\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource, Div\n",
    "\n",
    "def viz_smc_snapshot(\n",
    "    pf,\n",
    "    top_k=20,\n",
    "    html_path=None,\n",
    "    title=\"SMC Snapshot\",\n",
    "    history=None,   # dict from collect_history_simple\n",
    "    X=None, y=None  # if provided, compute LL bars for final top trees\n",
    "):\n",
    "    # ----- final snapshot (posterior over structures) -----\n",
    "    w = np.asarray(pf.W, float); w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "\n",
    "    def tree_key(nodes):  # permutation-invariant key\n",
    "        return frozenset(nodes)\n",
    "\n",
    "    mass_by_tree = defaultdict(float)\n",
    "    for i, (nodes, _open) in enumerate(pf.X):\n",
    "        mass_by_tree[tree_key(nodes)] += float(w[i])\n",
    "\n",
    "    ranked = sorted(mass_by_tree.items(), key=lambda kv: kv[1], reverse=True)[:top_k]\n",
    "    top_keys = [k for (k, m) in ranked]\n",
    "    probs    = [m for (k, m) in ranked]\n",
    "    # Make displayable tuple-of-splits for the top trees\n",
    "    top_trees = [tuple(sorted(k)) for k in top_keys]\n",
    "    labels    = [f\"T{k}\" for k in range(len(ranked))]\n",
    "\n",
    "    def fmt_tree(tree):\n",
    "        try:\n",
    "            return \"<br>\".join(str((tuple(A), tuple(B))) for (A,B) in tree)\n",
    "        except Exception:\n",
    "            return str(tree)\n",
    "\n",
    "    figs = []\n",
    "\n",
    "    # Posterior mass (top-k)\n",
    "    src1 = ColumnDataSource(dict(id=labels, p=probs))\n",
    "    p1 = figure(x_range=labels, title=f\"Posterior mass (top {len(labels)})\", width=720, height=220,\n",
    "                tools=\"hover\", tooltips=[(\"id\",\"@id\"),(\"mass\",\"@p{0.000}\")])\n",
    "    p1.vbar(x=\"id\", top=\"p\", source=src1, width=0.9)\n",
    "    figs.append(p1)\n",
    "\n",
    "    # Sorted particle weights (final)\n",
    "    sw = np.sort(w)[::-1]\n",
    "    src2 = ColumnDataSource(dict(rank=np.arange(sw.size), w=sw))\n",
    "    p2 = figure(title=\"Sorted particle weights (final)\", width=720, height=200,\n",
    "                tools=\"xpan,xwheel_zoom,reset,hover\",\n",
    "                tooltips=[(\"rank\",\"@rank\"),(\"w\",\"@w{0.0000}\")])\n",
    "    p2.line(\"rank\",\"w\", source=src2, line_width=2)\n",
    "    p2.circle(\"rank\",\"w\", source=src2, size=3, alpha=0.7)\n",
    "    figs.append(p2)\n",
    "\n",
    "    # ----- history (if provided) -----\n",
    "    history_div = None\n",
    "    if history is not None and len(history.get(\"ESS\", [])):\n",
    "        ESS_ts  = np.asarray(history[\"ESS\"], float)\n",
    "        uniq_ts = np.asarray(history[\"uniq\"], float)\n",
    "        t = np.arange(1, ESS_ts.size + 1)\n",
    "        src_ts = ColumnDataSource(dict(t=t, ESS=ESS_ts, uniq=uniq_ts))\n",
    "\n",
    "        # ESS and #unique trees over time\n",
    "        p3 = figure(title=\"ESS & #unique trees over time\", width=720, height=200,\n",
    "                    tools=\"xpan,xwheel_zoom,reset,hover\",\n",
    "                    tooltips=[(\"t\",\"@t\"),(\"ESS\",\"@ESS{0.0}\"),(\"#uniq\",\"@uniq\")])\n",
    "        p3.line(\"t\",\"ESS\",  source=src_ts, line_width=2, legend_label=\"ESS\")\n",
    "        p3.circle(\"t\",\"ESS\", source=src_ts, size=3, alpha=0.7)\n",
    "        p3.line(\"t\",\"uniq\", source=src_ts, line_width=2, line_dash=\"dashed\", legend_label=\"#unique\")\n",
    "        p3.circle(\"t\",\"uniq\", source=src_ts, size=3, alpha=0.7)\n",
    "        p3.legend.location = \"top_right\"\n",
    "        figs.append(p3)\n",
    "\n",
    "        # Mass over time for final top-3 trees\n",
    "        X_t, W_t = history[\"X_t\"], history[\"W_t\"]\n",
    "        top_show = top_keys[:min(3, len(top_keys))]\n",
    "        data_m = {\"t\": list(range(1, len(X_t) + 1))}\n",
    "        for k, key_ref in enumerate(top_show):\n",
    "            m_ts = []\n",
    "            for Xt, Wt in zip(X_t, W_t):\n",
    "                by = defaultdict(float)\n",
    "                for i, (nodes, _ol) in enumerate(Xt):\n",
    "                    by[tree_key(nodes)] += float(Wt[i])\n",
    "                m_ts.append(by.get(key_ref, 0.0))\n",
    "            data_m[f\"m{k}\"] = m_ts\n",
    "        src_m = ColumnDataSource(data_m)\n",
    "        p4 = figure(title=\"Mass over time (final top trees, up to 3)\", width=720, height=200,\n",
    "                    tools=\"xpan,xwheel_zoom,reset,hover\", tooltips=[(\"t\",\"@t\")])\n",
    "        for k in range(len(top_show)):\n",
    "            p4.line(\"t\", f\"m{k}\", source=src_m, line_width=2, legend_label=labels[k])\n",
    "        p4.legend.location = \"top_right\"\n",
    "        figs.append(p4)\n",
    "\n",
    "        # Modal tree per step (simple history list)\n",
    "        items = []\n",
    "        for ti, (Xt, Wt) in enumerate(zip(X_t, W_t), start=1):\n",
    "            by = defaultdict(float)\n",
    "            for i, (nodes, _ol) in enumerate(Xt):\n",
    "                by[tree_key(nodes)] += float(Wt[i])\n",
    "            if by:\n",
    "                key_mode, pmode = max(by.items(), key=lambda kv: kv[1])\n",
    "                Tmode = tuple(sorted(key_mode))\n",
    "                items.append(f\"<li>t={ti}: p={pmode:.3f} <code>{fmt_tree(Tmode)}</code></li>\")\n",
    "        history_div = Div(text=f\"<h4>History: modal tree per step</h4><ol>{''.join(items)}</ol>\")\n",
    "\n",
    "    # ----- optional: bars of log-likelihood for final top trees -----\n",
    "    best_ll_vals = None\n",
    "    if (X is not None) and (y is not None) and len(top_trees):\n",
    "        def tree_ll(tree):\n",
    "            try: return float(sum(split_loglik_star_sm(X, y, s) for s in tree))\n",
    "            except Exception: return np.nan\n",
    "        best_ll_vals = [tree_ll(t) for t in top_trees]\n",
    "        src_ll = ColumnDataSource(dict(id=labels, ll=best_ll_vals))\n",
    "        pLL = figure(x_range=labels, title=\"Log-likelihood (final top trees)\", width=720, height=200,\n",
    "                     tools=\"hover\", tooltips=[(\"id\",\"@id\"),(\"ll\",\"@ll{0.000}\")])\n",
    "        pLL.vbar(x=\"id\", top=\"ll\", source=src_ll, width=0.9)\n",
    "        figs.append(pLL)\n",
    "\n",
    "    # ----- summary -----\n",
    "    ESS_final = 1.0 / float((w**2).sum()) if w.size else 0.0\n",
    "    lst = \"\".join(f\"<li><b>{labels[i]}</b> (p={probs[i]:.3f}\"\n",
    "                  + (\"\" if (best_ll_vals is None) else f\", ll={best_ll_vals[i]:.3f}\")\n",
    "                  + f\")<br><code>{fmt_tree(top_trees[i])}</code></li>\"\n",
    "                  for i in range(len(labels)))\n",
    "    div_summary = Div(text=f\"<h3>{title}</h3>\"\n",
    "                           f\"<p>Particles: {w.size} &nbsp; | &nbsp; ESS(final): {ESS_final:.1f}</p>\"\n",
    "                           f\"<h4>Top trees</h4><ol>{lst}</ol>\")\n",
    "\n",
    "    layout = column(*figs, div_summary, *( [history_div] if history_div is not None else [] ))\n",
    "    if html_path:\n",
    "        output_file(html_path, title=title); save(layout)\n",
    "    else:\n",
    "        show(layout)\n",
    "\n",
    "    return {\"top_trees\": top_trees, \"top_probs\": probs, \"ESS\": ESS_final, \"ll_top\": best_ll_vals}\n",
    "\n",
    "def collect_history_simple(fk, N=256, resampling='systematic', ESSrmin=0.5):\n",
    "    \"\"\"\n",
    "    Runs SMC step-by-step once to capture history for plotting.\n",
    "    Returns: pf, history dict with ESS, uniq, X_t, W_t.\n",
    "    \"\"\"\n",
    "    pf = particles.SMC(fk=fk, N=N, resampling=resampling, ESSrmin=ESSrmin, store_history=False)\n",
    "    pf.initialise()\n",
    "    ESS, uniq, X_t, W_t = [], [], [], []\n",
    "    for _ in range(fk.T):\n",
    "        pf.step()\n",
    "        w = np.asarray(pf.W, float); w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "        ESS.append(1.0 / float((w**2).sum()))\n",
    "        uniq.append(len({frozenset(s[0]) for s in pf.X}))  # permutation-invariant unique\n",
    "        X_t.append(list(pf.X))       # states at time t: list of (nodes, open_leaves)\n",
    "        W_t.append(w.copy())         # weights at time t\n",
    "    return pf, {\"ESS\": np.array(ESS), \"uniq\": np.array(uniq), \"X_t\": X_t, \"W_t\": W_t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top trees (first 3 shown): [(((0, 1, 2, 3, 4, 6), (5,)), ((0, 1, 2, 3, 4), (6,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((1,), (0,))), (((0, 1, 2, 3, 4, 6), (5,)), ((0, 1, 2, 3, 4), (6,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((1,), (0,))), (((0, 1, 2, 3, 4, 6), (5,)), ((0, 1, 2, 3, 4), (6,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((1,), (0,)))]\n",
      "Best tree nodes: [((0, 1, 2, 3, 4, 6), (5,)), ((0, 1, 2, 3, 4), (6,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((1,), (0,))]\n",
      "The best tree is NOT found by particle filtering.\n",
      "Best tree is: (((0, 1, 2, 3, 4, 5), (6,)), ((0, 1, 2, 3, 4), (5,)), ((0, 1, 2, 3), (4,)), ((0, 1, 2), (3,)), ((0, 1), (2,)), ((1,), (0,)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, particles\n",
    "from numpy.random import Generator, PCG64\n",
    "\n",
    "def _canon(A, B):\n",
    "    A, B = tuple(sorted(A)), tuple(sorted(B))\n",
    "    return (A, B) if A < B else (B, A)\n",
    "\n",
    "def _uniform_partition(leaf, rng):\n",
    "    # uniform over unordered non-trivial splits using Python's random\n",
    "    number_of_classes = len(leaf)\n",
    "    # left_split_size = random.choice(range(1, number_of_classes))\n",
    "    left_split_size = rng.choice(range(1, number_of_classes))\n",
    "    m = rng.choice(list(leaf), size=left_split_size, replace=False)\n",
    "    new_L = sorted(list(m))\n",
    "    new_R = sorted(list(set(leaf) - set(new_L)))\n",
    "    return _canon(tuple(new_L), tuple(new_R))\n",
    "\n",
    "def _ll_nodes(Xb, yb, nodes):\n",
    "    return sum(split_loglik_star_sm(Xb, yb, s) for s in nodes)\n",
    "\n",
    "def _close_pair_leaves(nodes, open_leaves):\n",
    "    \"\"\"\n",
    "    Deterministically finish any open leaves of size 2:\n",
    "    e.g. (3,4) -> add ((3,), (4,)) to nodes and remove the leaf from open_leaves.\n",
    "    Returns (nodes_list, open_leaves_list).\n",
    "    \"\"\"\n",
    "    nodes = list(nodes)\n",
    "    ol = list(open_leaves)\n",
    "    while True:\n",
    "        idx2 = [i for i, L in enumerate(ol) if len(L) == 2]\n",
    "        if not idx2:\n",
    "            break\n",
    "        for i in reversed(idx2):\n",
    "            a, b = tuple(sorted(ol.pop(i)))\n",
    "            nodes.append(_canon((a,), (b,)))\n",
    "    return nodes, ol\n",
    "    \n",
    "class NDFK_LBL(particles.FeynmanKac):\n",
    "    \"\"\"Layer-by-layer SMC: one uniform structural split per step; weights = incremental LL.\"\"\"\n",
    "    def __init__(self, X, y, batches=10, n_classes=None, seed=42, rho=1.0):\n",
    "        self.X = np.asarray(getattr(X, 'values', X))\n",
    "        self.y = np.asarray(getattr(y, 'values', y))\n",
    "        self.K = n_classes or int(np.unique(self.y).size)\n",
    "        # self.T = max(0, self.K - 1)\n",
    "        self.T = max(0, self.K *2)\n",
    "        self.rng = Generator(PCG64(seed))\n",
    "        idx = np.arange(len(self.y)); self.rng.shuffle(idx)\n",
    "        self.batches = [np.asarray(b, int) for b in (np.array_split(idx, batches) if isinstance(batches, int) else batches)]\n",
    "        self.B = len(self.batches)\n",
    "\n",
    "        self.rho = rho\n",
    "        self.rho_sched = np.linspace(0.00000001, 0.2, self.T) if self.T>0 else np.array([1.0])\n",
    "\n",
    "    def M0(self, N): \n",
    "        root = tuple(range(self.K))\n",
    "        x0 = ((), (root,))  \n",
    "        return np.array([x0] * N, dtype=object)\n",
    "\n",
    "    def M(self, t, xp):\n",
    "        out = []\n",
    "        for nodes, open_leaves in xp:\n",
    "            nodes, ol = _close_pair_leaves(nodes, open_leaves)\n",
    "            if ol:\n",
    "                j = int(self.rng.integers(0, len(ol)))\n",
    "                leaf = ol.pop(j)\n",
    "                A, B = _uniform_partition(leaf, self.rng)\n",
    "                nodes = tuple(list(nodes) + [(A, B)])\n",
    "                if len(A) > 1: \n",
    "                    ol.append(A)\n",
    "                if len(B) > 1: \n",
    "                    ol.append(B)\n",
    "                ol = tuple(sorted(ol))\n",
    "            else: \n",
    "                nodes, _   = local_swap(nodes, self.rng)\n",
    "            nodes, ol = _close_pair_leaves(nodes, ol)\n",
    "            out.append((nodes, ol))\n",
    "        return np.asarray(out, dtype=object)\n",
    "\n",
    "    def logG(self, t, xp, x):\n",
    "        I = self.batches[t % self.B]\n",
    "        Xb, yb = self.X[I], self.y[I]\n",
    "        # if xp is None:\n",
    "        #     prev_ll = np.zeros(len(x), dtype=float)\n",
    "        # else:\n",
    "        #     prev_ll = np.array([_ll_nodes(Xb, yb, n) for (n, _) in xp], float)\n",
    "        curr_ll = np.array([_ll_nodes(Xb, yb, n) for (n, _) in x], float)\n",
    "        rho_t = float(self.rho_sched[min(t, len(self.rho_sched)-1)])\n",
    "        return rho_t * (curr_ll)\n",
    "        # return self.rho * (curr_ll - prev_ll)\n",
    "\n",
    "\n",
    "# ---- quick run  ----\n",
    "fk = NDFK_LBL(X, y, batches=10, seed=43, rho=0.05)\n",
    "# pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.1, store_history=False)\n",
    "pf = particles.SMC(fk=fk, N=256, resampling='residual', ESSrmin=0.05, store_history=False)\n",
    "\n",
    "# pf = particles.SMC(fk=fk, N=256, resampling='stratified', ESSrmin=0.9, store_history=False)\n",
    "\n",
    "# pf = particles.SMC(fk=fk, N=384, resampling='residual', ESSrmin=0.25)\n",
    "\n",
    "pf.run()\n",
    "idx = np.argsort(pf.W)[::-1]\n",
    "states = [pf.X[i] for i in idx]          \n",
    "best_nodes, best_open = states[0]\n",
    "\n",
    "# top_k node-sets (trees)\n",
    "top_k = 20\n",
    "top_states = states[:top_k]\n",
    "top_trees = [bfs_splits(s[0]) for s in top_states]   \n",
    "\n",
    "print(\"Top trees (first 3 shown):\", top_trees[:3])\n",
    "print(\"Best tree nodes:\", best_nodes)\n",
    "\n",
    "if bfs_splits(best_prev_tree) in top_trees:\n",
    "    print(\"The best tree is found inside top trees!\")\n",
    "else:\n",
    "    print(\"The best tree is NOT found by particle filtering.\")\n",
    "    print(\"Best tree is:\", best_prev_tree)\n",
    "\n",
    "# summary = viz_smc_snapshot(pf, top_k=20, html_path=None, title=\"ND-SMC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e4e7632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
      "BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
      "BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'top_trees': [(((0, 1), (2,)),\n",
       "   ((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (6,)),\n",
       "   ((0, 1, 2, 3, 4, 6), (5,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (2,)),\n",
       "   ((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (6,)),\n",
       "   ((0, 1, 2, 3, 4, 6), (5,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 3, 5), (4,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1, 2), (3,)),\n",
       "   ((1, 2, 3), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (2,)),\n",
       "   ((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (6,)),\n",
       "   ((0, 1, 2, 3, 5, 6), (4,)),\n",
       "   ((0, 1, 2, 3, 6), (5,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (6,)),\n",
       "   ((0, 1, 2, 3, 5, 6), (4,)),\n",
       "   ((0, 1, 2, 3, 6), (5,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (2,)),\n",
       "   ((0, 1, 2), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 3, 5), (4,)),\n",
       "   ((0, 1, 2, 5), (3,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 2), (1,)),\n",
       "   ((2,), (0,))),\n",
       "  (((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1, 2, 3), (0,)),\n",
       "   ((1, 3), (2,)),\n",
       "   ((3,), (1,))),\n",
       "  (((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1, 2), (4,)),\n",
       "   ((1, 2, 3, 4), (0,)),\n",
       "   ((1, 2, 4), (3,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (2,)),\n",
       "   ((0, 1, 2), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 4), (3,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 4), (3,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((1, 2), (5,)),\n",
       "   ((1, 2, 3, 4, 5), (0,)),\n",
       "   ((1, 2, 3, 5), (4,)),\n",
       "   ((1, 2, 5), (3,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (3,)),\n",
       "   ((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 3), (2,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2, 3), (4,)),\n",
       "   ((0, 1, 2, 3, 4), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 3), (2,)),\n",
       "   ((1, 3), (0,)),\n",
       "   ((3,), (1,))),\n",
       "  (((0, 1), (3,)),\n",
       "   ((0, 1, 2, 3), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 3, 5), (4,)),\n",
       "   ((0, 1, 3), (2,)),\n",
       "   ((1,), (0,))),\n",
       "  (((0, 1, 2), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 4, 5), (3,)),\n",
       "   ((0, 1, 2, 5), (4,)),\n",
       "   ((1, 2), (0,)),\n",
       "   ((2,), (1,))),\n",
       "  (((0, 1), (2,)),\n",
       "   ((0, 1, 2), (5,)),\n",
       "   ((0, 1, 2, 3, 4, 5), (6,)),\n",
       "   ((0, 1, 2, 4, 5), (3,)),\n",
       "   ((0, 1, 2, 5), (4,)),\n",
       "   ((1,), (0,)))],\n",
       " 'top_probs': [0.9213248268762849,\n",
       "  0.030674801324509084,\n",
       "  0.021580384217353036,\n",
       "  0.009921136592717998,\n",
       "  0.008511132351552425,\n",
       "  0.007002150171484287,\n",
       "  0.000537480952015195,\n",
       "  0.00044565877036384567,\n",
       "  1.2278861621843202e-06,\n",
       "  1.0107491847004973e-06,\n",
       "  9.757683406789612e-08,\n",
       "  3.418695094757125e-08,\n",
       "  2.980922105556277e-08,\n",
       "  2.4716672751514796e-08,\n",
       "  1.8919424511260157e-09,\n",
       "  1.622879787874631e-09,\n",
       "  1.5350963178823004e-10,\n",
       "  8.701463118780188e-11,\n",
       "  3.2318186364471156e-11,\n",
       "  3.031538804222511e-11],\n",
       " 'ESS': 25.43347439237776,\n",
       " 'll_top': [-124616.17004920791,\n",
       "  -124701.2862850109,\n",
       "  -124642.28245536807,\n",
       "  -124675.17387885074,\n",
       "  -124776.7914750624,\n",
       "  -124778.90970890332,\n",
       "  -124968.18626072076,\n",
       "  -125027.19009036358,\n",
       "  -125072.15310036635,\n",
       "  -125277.78532355311,\n",
       "  -125235.14953485808,\n",
       "  -125369.17281643135,\n",
       "  -125356.90153263087,\n",
       "  -125415.90536227368,\n",
       "  -125631.119967585,\n",
       "  -125493.44944232046,\n",
       "  -125652.77446571372,\n",
       "  -125595.06703853213,\n",
       "  -125721.56917279304,\n",
       "  -125662.56534315021]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pf as usual\n",
    "pf = particles.SMC(fk=fk, N=256, resampling='residual', ESSrmin=0.05, store_history=False)\n",
    "# pf = particles.SMC(fk=fk, N=256, resampling='systematic', ESSrmin=0.4, store_history=False)\n",
    "# pf = particles.SMC(fk=fk, N=384, resampling='residual', ESSrmin=0.25)\n",
    "\n",
    "# Step and record\n",
    "ESS, uniq, X_t, W_t = [], [], [], []\n",
    "for _ in range(fk.T):\n",
    "    pf.next()\n",
    "    w = np.asarray(pf.W, float); \n",
    "    w /= (w.sum() or 1.0)\n",
    "    ESS.append(1.0 / (w**2).sum())\n",
    "    uniq.append(len({frozenset(s[0]) for s in pf.X}))  # permutation-invariant unique structures\n",
    "    X_t.append(list(pf.X))                              # states this step\n",
    "    W_t.append(w.copy())                                # weights this step\n",
    "\n",
    "history = {\"ESS\": np.array(ESS), \"uniq\": np.array(uniq), \"X_t\": X_t, \"W_t\": W_t}\n",
    "\n",
    "# Now viz (same function you already have)\n",
    "viz_smc_snapshot(pf, top_k=20, title=\"ND-SMC (with history)\", history=history, X=X, y=y)\n",
    "\n",
    "# viz_smc_snapshot(pf, top_k=20, title=\"ND-SMC (with history)\", history=hist, X=X, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
