{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad5ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from scipy.linalg import toeplitz\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import zlib\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, cross_validate, StratifiedKFold\n",
    "\n",
    "# Load .env from the project root (adjust if needed)\n",
    "env_path = \".env\"\n",
    "load_dotenv(dotenv_path=env_path, override=False)  # don't clobber real env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "749b45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_mnlogit(\n",
    "    n_samples: int,\n",
    "    n_classes: int,\n",
    "    n_features: int,\n",
    "    *,\n",
    "    base_rates=None,          # e.g. [0.6,0.3,0.1]; if None => uniform\n",
    "    coef_scale: float = 1.0,  # std for coefficients\n",
    "    class_sep: float = 1.0,   # scales logits (↑ easier, ↓ harder)\n",
    "    intercept: bool = True,\n",
    "    n_informative: int | None = None,  # if None => all features informative\n",
    "    feature_scale: float = 1.0,\n",
    "    flip_y: float = 0.0,               # label noise fraction in [0,1]\n",
    "    seed: int | None = None,           # if None => hash of params (deterministic)\n",
    "    return_probas: bool = False,\n",
    "):\n",
    "    \"\"\"Deterministic multinomial-logit simulator with minimal dependencies.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ----- sizes / basic checks -----\n",
    "    K, p, n = int(n_classes), int(n_features), int(n_samples)\n",
    "    if K < 2: raise ValueError(\"n_classes must be >= 2\")\n",
    "    if p < 1: raise ValueError(\"n_features must be >= 1\")\n",
    "    n_inf = p if n_informative is None else int(n_informative)\n",
    "    if not (1 <= n_inf <= p): raise ValueError(\"n_informative must be in [1, n_features]\")\n",
    "\n",
    "    # ----- class priors -----\n",
    "    if base_rates is None:\n",
    "        pri = np.full(K, 1.0 / K)\n",
    "    else:\n",
    "        pri = np.asarray(base_rates, float)\n",
    "        if pri.shape != (K,) or not np.isclose(pri.sum(), 1.0):\n",
    "            raise ValueError(\"base_rates must be length K and sum to 1\")\n",
    "\n",
    "    # ----- features: Gaussian -----\n",
    "    X = rng.normal(0.0, feature_scale, size=(n, p))\n",
    "\n",
    "    # ----- parameters -----\n",
    "    W = np.zeros((p, K))\n",
    "    W[:n_inf, :] = rng.normal(0.0, coef_scale, size=(n_inf, K))\n",
    "    W *= class_sep\n",
    "    b = np.log(np.clip(pri, 1e-12, None)) - (0 if not intercept else np.mean(np.log(np.clip(pri,1e-12,None))))\n",
    "    if not intercept: b = np.zeros(K)\n",
    "\n",
    "    # ----- probabilities & labels -----\n",
    "    P = softmax(X @ W + b, axis=1)\n",
    "    cumP = np.cumsum(P, axis=1)\n",
    "    y = (rng.random(n)[:, None] > cumP).sum(axis=1).astype(int)\n",
    "\n",
    "    # optional label noise (flip to a different class uniformly)\n",
    "    if flip_y > 0:\n",
    "        flips = rng.random(n) < float(flip_y)\n",
    "        if flips.any():\n",
    "            r = rng.integers(0, K-1, size=flips.sum())\n",
    "            yf = y[flips]\n",
    "            y[flips] = r + (r >= yf)\n",
    "\n",
    "    df = pd.DataFrame(X, columns=[f\"X{j}\" for j in range(p)])\n",
    "    df[\"Y\"] = y + 1\n",
    "    if return_probas:\n",
    "        for k in range(K): df[f\"p{k}\"] = P[:, k]\n",
    "\n",
    "    params = {\"W\": W, \"b\": b, \"seed_used\": seed}\n",
    "    return df, params\n",
    "\n",
    "# --- set values ---\n",
    "seed=42\n",
    "n_samples=500\n",
    "n_classes=4\n",
    "n_features=100\n",
    "base_rates=[0.5, 0.2, 0.2, 0.1]\n",
    "coef_scale=0.7\n",
    "class_sep=1.2\n",
    "n_informative=8\n",
    "flip_y=0.05\n",
    "# --- set values ---\n",
    "\n",
    "\n",
    "# return_probas=False\n",
    "\n",
    "# df, params = simulate_mnlogit(\n",
    "#     seed=seed,\n",
    "#     n_samples=n_samples, n_classes=n_classes, n_features=n_features,\n",
    "#     base_rates=base_rates,\n",
    "#     coef_scale=coef_scale, class_sep=class_sep,\n",
    "#     n_informative=n_informative, flip_y=flip_y,\n",
    "#     return_probas=False\n",
    "# )\n",
    "# print(df.head(), \"\\nW:\", params[\"W\"].shape, \"b:\", params[\"b\"].shape, \"seed:\", params[\"seed_used\"])\n",
    "\n",
    "# df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns: \n",
    "    # seed=42,\n",
    "    # n_samples=500, n_classes=4, n_features=100,\n",
    "    # base_rates=[0.5, 0.2, 0.2, 0.1],\n",
    "    # coef_scale=0.7, class_sep=1.2,\n",
    "    # n_informative=8, flip_y=0.05,\n",
    "    # return_probas=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c241211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, log_loss, balanced_accuracy_score,\n",
    "                             cohen_kappa_score, matthews_corrcoef, jaccard_score, hamming_loss)\n",
    "\n",
    "def df_upsert_postgres(data_frame, table_name, engine, schema=None, match_columns=None, insert_only=False):\n",
    "    \"\"\"\n",
    "    Perform an \"upsert\" on a PostgreSQL table from a DataFrame.\n",
    "    Constructs an INSERT … ON CONFLICT statement, uploads the DataFrame to a\n",
    "    temporary table, and then executes the INSERT.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_frame : pandas.DataFrame\n",
    "        The DataFrame to be upserted.\n",
    "    table_name : str\n",
    "        The name of the target table.\n",
    "    engine : sqlalchemy.engine.Engine\n",
    "        The SQLAlchemy Engine to use.\n",
    "    schema : str, optional\n",
    "        The name of the schema containing the target table.\n",
    "    match_columns : list of str, optional\n",
    "        A list of the column name(s) on which to match. If omitted, the\n",
    "        primary key columns of the target table will be used.\n",
    "    insert_only : bool, optional\n",
    "        On conflict do not update. (Default: False)\n",
    "    \"\"\"\n",
    "    table_spec = \"\"\n",
    "    if schema:\n",
    "        table_spec += '\"' + schema.replace('\"', '\"\"') + '\".'\n",
    "    table_spec += '\"' + table_name.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "\n",
    "    df_columns = list(data_frame.columns)\n",
    "    if not match_columns:\n",
    "        insp = sa.inspect(engine)\n",
    "        match_columns = insp.get_pk_constraint(table_name, schema=schema)[\n",
    "            \"constrained_columns\"\n",
    "        ]\n",
    "    columns_to_update = [col for col in df_columns if col not in match_columns]\n",
    "    insert_col_list = \", \".join([f'\"{col_name}\"' for col_name in df_columns])\n",
    "    stmt = f\"INSERT INTO {table_spec} ({insert_col_list})\\n\"\n",
    "    stmt += f\"SELECT {insert_col_list} FROM temp_table\\n\"\n",
    "    match_col_list = \", \".join([f'\"{col}\"' for col in match_columns])\n",
    "    stmt += f\"ON CONFLICT ({match_col_list}) DO \"\n",
    "    if insert_only:\n",
    "        stmt += \"NOTHING\"\n",
    "    else:\n",
    "        stmt += \"UPDATE SET\\n\"\n",
    "        stmt += \", \".join(\n",
    "            [f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns_to_update]\n",
    "        )\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(\"DROP TABLE IF EXISTS temp_table\")\n",
    "        conn.exec_driver_sql(\n",
    "            f\"CREATE TEMPORARY TABLE temp_table AS SELECT * FROM {table_spec} WHERE false\"\n",
    "        )\n",
    "        data_frame.to_sql(\"temp_table\", conn, if_exists=\"append\", index=False)\n",
    "        conn.exec_driver_sql(stmt)\n",
    "\n",
    "def calculate_metrics(config, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    confusion_matrix_ls = str(confusion_matrix(y_true, y_pred))\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    matthews_corr_coef = matthews_corrcoef(y_true, y_pred)\n",
    "    jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
    "    hamming_loss_db = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"accuracy\":[accuracy],        \n",
    "        \"precision_macro\":[precision_macro], \n",
    "        \"precision_micro\":[precision_micro], \n",
    "        \"recall_macro\":[recall_macro], \n",
    "        \"recall_micro\":[recall_micro], \n",
    "        \"f1_macro\":[f1_macro], \n",
    "        \"f1_micro\":[f1_micro], \n",
    "        \"balanced_accuracy\":[balanced_accuracy], \n",
    "        \"cohen_kappa\":[cohen_kappa], \n",
    "        \"matthews_corrcoef\":[matthews_corr_coef], \n",
    "        \"jaccard_macro\":[jaccard_macro], \n",
    "        \"hamming_loss\":[hamming_loss_db], \n",
    "        \"confusion_matrix\":[confusion_matrix_ls],\n",
    "    }\n",
    "    return pd.DataFrame(metrics_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "96fb061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/7j0s980d2w7951jcy8zc25ww08y435/T/ipykernel_28033/3010355022.py:45: RuntimeWarning: divide by zero encountered in matmul\n",
      "  P = softmax(X @ W + b, axis=1)\n",
      "/var/folders/h2/7j0s980d2w7951jcy8zc25ww08y435/T/ipykernel_28033/3010355022.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  P = softmax(X @ W + b, axis=1)\n",
      "/var/folders/h2/7j0s980d2w7951jcy8zc25ww08y435/T/ipykernel_28033/3010355022.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  P = softmax(X @ W + b, axis=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "postgres_df = pd.DataFrame()\n",
    "save_db = True\n",
    "traversal_type = \"nd_stepwise\"\n",
    "underscored_model_types = \"a\"\n",
    "\n",
    "\n",
    "seed=42\n",
    "n_samples=1000\n",
    "n_classes=6\n",
    "n_features=50\n",
    "base_rates=[0.2, 0.2, 0.15, 0.15, 0.15, 0.15]\n",
    "coef_scale=0.9\n",
    "class_sep=1.2\n",
    "n_informative=25\n",
    "flip_y=0.001\n",
    "\n",
    "save_db = True\n",
    "\n",
    "df, params = simulate_mnlogit(\n",
    "    seed=seed,\n",
    "    n_samples=n_samples, n_classes=n_classes, n_features=n_features,\n",
    "    base_rates=base_rates,\n",
    "    coef_scale=coef_scale, class_sep=class_sep,\n",
    "    n_informative=n_informative, flip_y=flip_y,\n",
    "    return_probas=False\n",
    ")\n",
    "\n",
    "if save_db == True:\n",
    "    name = \"mlr_\" + str(n_classes) + \"_class_\" + str(n_features) + \"_\" + str(n_informative) + str(seed) + \"_\" + str(n_samples) + \"_\" + \"_\" + str(coef_scale) + \"_\" + str(flip_y) + \"_\" + str(class_sep) + \"_\" + str(base_rates).replace('[', '').replace(']', '').replace(' ','')\n",
    "    postgres_df.loc[0, \"simulation_id\"] = int(zlib.crc32(name.encode()))\n",
    "    postgres_df.loc[0, \"name\"] = name\n",
    "    postgres_df.loc[0, \"seed\"] = seed\n",
    "    postgres_df.loc[0, \"n_classes\"] = n_classes\n",
    "    postgres_df.loc[0, \"n_features\"] = n_features\n",
    "    postgres_df.loc[0, \"n_samples\"] = n_samples\n",
    "    postgres_df.loc[0, \"n_informative\"] = n_informative\n",
    "    postgres_df.loc[0, \"flip_y\"] = flip_y\n",
    "    postgres_df.loc[0, \"base_rates\"] = str(base_rates)\n",
    "    postgres_df.loc[0, \"coef_scale\"] = coef_scale\n",
    "    postgres_df.loc[0, \"class_sep\"] = class_sep\n",
    "    postgres_df.loc[0, \"notes\"] = \"\"\n",
    "    password = os.environ['ML_POSTGRESS_URL'].split(':')[2].split(\"@\")[0]\n",
    "    host = os.environ['ML_POSTGRESS_HOST']\n",
    "    database = \"simulation\"\n",
    "    engine = sa.create_engine( f\"postgresql://max:{password}@{host}\" + f\"/{database}\")\n",
    "    postgres_df.to_sql('mlr_simulations_dictionary', engine, schema=\"public\", if_exists='append', index=False)\n",
    "    # engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, X_full_train['Y'], stratify=X_full_train['Y'], test_size=0.3, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
